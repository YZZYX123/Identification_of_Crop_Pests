import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import SwinModel

# 动态注意力模块定义（保持与T版本一致）
class DynamicAttentionModule(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16):
        super(DynamicAttentionModule, self).__init__()
        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio)
        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: batch_size x num_patches x embed_dim
        attention = self.fc1(x)
        attention = F.relu(attention)
        attention = self.fc2(attention)
        attention = self.sigmoid(attention)
        return x * attention  # 逐元素相乘，增强关注区域

# 多尺度 Swin Transformer 模型
class MultiScaleSwin(nn.Module):
    def __init__(self, base_swin_model, num_scales=3, num_classes=100):
        super(MultiScaleSwin, self).__init__()
        self.swin = base_swin_model
        self.num_scales = num_scales
        
        # 获取Swin的隐藏层维度
        in_channels = self.swin.config.hidden_size
        
        # 动态注意力模块
        self.attention_modules = nn.ModuleList([
            DynamicAttentionModule(in_channels) for _ in range(num_scales)
        ])
        
        # 融合层（使用Conv1d处理序列特征）
        self.fusion_layer = nn.Conv1d(
            in_channels * num_scales,
            in_channels,
            kernel_size=1
        )
        
        # 分类头
        self.classifier = nn.Linear(in_channels, num_classes)

    def forward(self, x):
        # 获取Swin的patch特征
        outputs = self.swin(x)
        patch_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]
        
        # 多尺度特征学习
        features = []
        for scale_idx in range(self.num_scales):
            attention_output = self.attention_modules[scale_idx](patch_embeddings)
            features.append(attention_output)
        
        # 特征融合
        fused_features = torch.cat(features, dim=2)  # [batch, seq_len, hidden_size*num_scales]
        fused_features = fused_features.transpose(1, 2)  # [batch, hidden_size*num_scales, seq_len]
        
        # 通过1D卷积融合特征
        fused_features = self.fusion_layer(fused_features)  # [batch, hidden_size, seq_len]
        
        # 全局平均池化
        pooled_features = fused_features.mean(dim=2)  # [batch, hidden_size]
        
        # 分类
        return self.classifier(pooled_features)

# 示例测试
if __name__ == "__main__":
    # 输入示例
    x = torch.randn(1, 3, 224, 224)
    
    # 加载基础Swin模型（使用tiny版本作为示例）
    swin_model = SwinModel.from_pretrained("microsoft/swin-base-patch4-window7-224")
    
    # 创建多尺度模型
    model = MultiScaleSwin(swin_model, num_scales=3, num_classes=102)
    
    # 前向传播
    output = model(x)
    print("Output shape:", output.shape)  # 预期输出形状: [1, 102]
